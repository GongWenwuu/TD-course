{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Higher-Order Orthogonal Iteration for Tensor Decomposition and Completion\n",
    "\n",
    "This notebook shows how to implement a gHOI imputer on traffic data sets (i.e., Guangzhou traffic speed data). To overcome the problem of missing values within multivariate time series data, this method takes into account both tensor tucker decomposition and low rank core tensor structure. For an in-depth discussion of gHOI, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Yuanyuan Liu, Fanhua Shang, Wei Fan, James Cheng,  Hong Cheng (2014). <b>Generalized Higher-Order Orthogonal Iteration for Tensor Decomposition and Completion</b>. NIPS Proceedings. <a href=\"https://papers.nips.cc/paper/5476-generalized-higher-order-orthogonal-iteration-for-tensor-decomposition-and-completion.pdf\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numpy.linalg import inv as inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LATC-imputer kernel\n",
    "\n",
    "We start by introducing some necessary functions that relies on `Numpy`.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<ul>\n",
    "<li><b><code>ten2mat</code>:</b> <font color=\"black\">Unfold tensor as matrix by specifying mode.</font></li>\n",
    "<li><b><code>mat2ten</code>:</b> <font color=\"black\">Fold matrix as tensor by specifying dimension (i.e, tensor size) and mode.</font></li>\n",
    "<li><b><code>tucker_combine</code>:</b> <font color=\"black\">Combine core tensor and unitary matrices as full tensor.</font></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor folding and unfolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n",
    "\n",
    "def mat2ten(mat, dim, mode):\n",
    "    index = list()\n",
    "    index.append(mode)\n",
    "    for i in range(dim.shape[0]):\n",
    "        if i != mode:\n",
    "            index.append(i)\n",
    "    return np.moveaxis(np.reshape(mat, list(dim[index]), order = 'F'), 0, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor tucker combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tucker_combine(var, skip = False, skipn = 0):\n",
    "    annotation = 'qwertyuiop'\n",
    "    G = var[0]\n",
    "    R = G.shape\n",
    "    dim_N = len(R)\n",
    "    anno = annotation[:dim_N]\n",
    "    W = G.copy()\n",
    "    for n in range(len(var) - 1):\n",
    "        if skip == True and n == skipn:\n",
    "            continue\n",
    "        target = anno.replace(anno[n], 'n')\n",
    "        mul_type = anno + ', n' + annotation[n] + '->' + target\n",
    "        W = np.einsum(mul_type, W, var[n + 1])\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to randomly initiate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variables(dim, R):\n",
    "    G = np.random.rand(*R)\n",
    "    dim_N = len(dim)\n",
    "    U = []\n",
    "    for i in range(dim_N):\n",
    "        U.append(np.random.rand(dim[i], R[i]))\n",
    "        \n",
    "    V = []\n",
    "    for i in range(dim_N):\n",
    "        V.append(ten2mat(G, i))\n",
    "    \n",
    "    Y = []\n",
    "    for i in range(dim_N):\n",
    "        Y.append(np.zeros_like(V[i]))\n",
    "    return G, U, V, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losscal(V, Y, G, U, X, X_pre, mu, lambda_l):\n",
    "    loss = 0\n",
    "    dim_N = len(X.shape)\n",
    "    for i in range(dim_N):\n",
    "        u, s, v = np.linalg.svd(V[i], full_matrices=0)\n",
    "        mat = ten2mat(G, i) - V[i]\n",
    "        loss += np.sum(s) + np.einsum('ij, ij', Y[i], mat) + mu / 2 * np.sum(np.square(mat))\n",
    "    loss += lambda_l * np.sum(np.square(X - X_pre))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error calculator\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<ul>\n",
    "<li><b><code>compute_mape</code>:</b> <font color=\"black\">Compute the value of Mean Absolute Percentage Error (MAPE).</font></li>\n",
    "<li><b><code>compute_rmse</code>:</b> <font color=\"black\">Compute the value of Root Mean Square Error (RMSE).</font></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "> Note that $$\\mathrm{MAPE}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\left|y_{i}-\\hat{y}_{i}\\right|}{y_{i}} \\times 100, \\quad\\mathrm{RMSE}=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}},$$ where $n$ is the total number of estimated values, and $y_i$ and $\\hat{y}_i$ are the actual value and its estimation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function validity test\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<ul>\n",
    "<li><b><code>tucker_combination</code>:</b> <font color=\"black\">Test the function validity of `tucker_combination`</font></li>\n",
    "<li><b><code>padding</code>:</b> <font color=\"black\">Test the validity of tensor padding.</font></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tucker_combination test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 1)\n",
      "Func: tucker_combine result:\n",
      "[[[0.12230859]\n",
      "  [0.36446952]]\n",
      "\n",
      " [[0.21144424]\n",
      "  [0.6300864 ]]\n",
      "\n",
      " [[0.03812287]\n",
      "  [0.11360301]]]\n",
      "\n",
      "Ground Truth:\n",
      "[[[0.12230859]\n",
      "  [0.36446952]]\n",
      "\n",
      " [[0.21144424]\n",
      "  [0.6300864 ]]\n",
      "\n",
      " [[0.03812287]\n",
      "  [0.11360301]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "G = np.random.rand(1, 2, 3)\n",
    "U_1 = np.random.rand(3, 1)\n",
    "U_2 = np.random.rand(2, 2)\n",
    "U_3 = np.random.rand(1, 3)\n",
    "var = [G, U_1, U_2, U_3]\n",
    "tucker_cb = tucker_combine(var)\n",
    "print(tucker_cb.shape)\n",
    "dim1 = U_1.shape[0]\n",
    "dim2 = U_2.shape[0]\n",
    "dim3 = U_3.shape[0]\n",
    "\n",
    "R1 = U_1.shape[1]\n",
    "R2 = U_2.shape[1]\n",
    "R3 = U_3.shape[1]\n",
    "\n",
    "GU = np.zeros((dim1, R2, R3))\n",
    "for i in range(dim1):\n",
    "    for j in range(R2):\n",
    "        for k in range(R3):\n",
    "            GU[i,j,k] = np.matmul(G[:, j, k], U_1[i, :])\n",
    "\n",
    "GUU = np.zeros((dim1, dim2, R3))\n",
    "for i in range(dim1):\n",
    "    for j in range(dim2):\n",
    "        for k in range(R3):\n",
    "            GUU[i,j,k] = np.matmul(GU[i, :, k], U_2[j, :])\n",
    "            \n",
    "GUUU = np.zeros((dim1, dim2, dim3))\n",
    "for i in range(dim1):\n",
    "    for j in range(dim2):\n",
    "        for k in range(dim3):\n",
    "            GUUU[i,j,k] = np.matmul(GUU[i, j, :], U_3[k, :])\n",
    "print('Func: tucker_combine result:')\n",
    "print(tucker_cb)\n",
    "print()\n",
    "print('Ground Truth:')\n",
    "print(GUUU)\n",
    "print()\n",
    "\n",
    "# print(tucker_cb.dtype)\n",
    "# print(GUUU.dtype)\n",
    "# print()\n",
    "# print('Dose tucker_cb equal GUUU?')\n",
    "# if np.array_equal(GUUU, tucker_cb):\n",
    "#     print('Yes!')\n",
    "# else:\n",
    "#     print('No~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 1]\n",
      "  [2 2]\n",
      "  [0 0]]\n",
      "\n",
      " [[3 3]\n",
      "  [4 4]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]]]\n"
     ]
    }
   ],
   "source": [
    "# A = np.zeros((5,5))\n",
    "B = np.array([[[1,1],[2,2]],[[3,3],[4,4]]])\n",
    "delta = np.array([2, 1, 0])\n",
    "N_tuple = ()\n",
    "for i in range(len(delta)):\n",
    "    N_tuple += ((0, delta[i]), )\n",
    "C = np.pad(B, N_tuple, 'constant', constant_values=(0))\n",
    "# C = B.resize((5, 5))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Higher-Order Orthogonal Iteration Imputer\n",
    "The following `imputer` kernel includes some necessary inputs:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<ul>\n",
    "<li><b><code>dense_tensor</code>:</b> <font color=\"black\">This is an input which has the ground truth for validation. If this input is not available, you could use <code>dense_tensor = sparse_tensor.copy()</code> instead.</font></li>\n",
    "<li><b><code>sparse_tensor</code>:</b> <font color=\"black\">This is a partially observed tensor which has many missing entries.</font></li>\n",
    "<li><b><code>r</code>:</b> <font color=\"black\">Initial n-rank of the aprroximated tensor, e.g., <code>r = np.array([10, 10, 10])</code>. </font></li>\n",
    "<li><b><code>R_max</code>:</b> <font color=\"black\">The upper bound of the approximated tensor, e.g., <code>R_max = np.array([80, 80, 80])</code>. </font></li>\n",
    "<li><b><code>lambda_l</code>:</b> <font color=\"black\">Weight for sum of squared residual error  e.g., <code>lambda_l = 1</code>. </font></li>\n",
    "<li><b><code>rho</code>:</b> <font color=\"black\">Scalling factor of mu, e.g., <code>epsilon = 1.01</code>. </font></li>\n",
    "<li><b><code>mu0</code>:</b> <font color=\"black\">Initial learning rate for ADMM, e.g., <code>mu0 = 0.0005</code>. </font></li>\n",
    "<li><b><code>mu_max</code>:</b> <font color=\"black\">Upper bound of learning rate for ADMM, e.g., <code>mu_max = 0.01</code>. </font></li>\n",
    "<li><b><code>delta</code>:</b> <font color=\"black\">Rank increasing step lengths, e.g., <code>delta = np.array([5, 5, 5])</code>. </font></li>\n",
    "<li><b><code>epsilon</code>:</b> <font color=\"black\">Rank increasing criteria, e.g., <code>epsilon = 0.2 </code>. </font></li>\n",
    "<li><b><code>maxiter</code>:</b> <font color=\"black\">Maximum iteration to stop algorithm, e.g., <code>maxiter = 100 </code>. </font></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer(dense_tensor, sparse_tensor, r, R_max, lambda_l, rho, mu0, mu_min, delta, epsilon, maxiter):\n",
    "    X = sparse_tensor.copy()\n",
    "    mu = mu0\n",
    "    R = r.copy()\n",
    "    dim = sparse_tensor.shape\n",
    "    dim_N = len(dim)\n",
    "    G, U, V, Y = init_variables(dim, R)\n",
    "    pos = np.where(sparse_tensor == 0)\n",
    "    pos_test = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "    start_time = time.time()\n",
    "    L_pre = np.inf\n",
    "    for iteration in range(maxiter):\n",
    "        # Update unitary matrices\n",
    "        for i in range(dim_N):\n",
    "            parse_list = [X]\n",
    "            for j in range(dim_N):\n",
    "                    parse_list.append(U[j].T)\n",
    "            M = tucker_combine(parse_list, skip = True, skipn = i)\n",
    "            N = np.zeros(R)\n",
    "            for j in range(dim_N):\n",
    "                N = N + mat2ten(V[j]-Y[j]/mu, R, j)\n",
    "            MN = np.matmul(ten2mat(M, i), ten2mat(N, i).T)\n",
    "            Ud, sd, Vd = np.linalg.svd(MN, full_matrices=0)\n",
    "            U[i] = np.matmul(Ud, Vd)\n",
    "        \n",
    "        # Update core tensor\n",
    "        parse_list = [X]\n",
    "        for j in range(dim_N):\n",
    "            parse_list.append(U[j].T)\n",
    "        G = lambda_l / (lambda_l + dim_N * mu) * tucker_combine(parse_list, skip = False)\n",
    "        for j in range(dim_N):\n",
    "            G = G + mu / (lambda_l + dim_N * mu) * mat2ten(V[j]-Y[j]/mu, R, j)\n",
    "        \n",
    "        # Update auxiliary matrices\n",
    "        for i in range(dim_N):\n",
    "            Us, ss, Vs = np.linalg.svd(ten2mat(G, i) + Y[i]/mu, full_matrices=0)\n",
    "            vec = ss - 1 / mu\n",
    "            vec[vec <= 0] = 0\n",
    "            V[i] = np.matmul(np.matmul(Us, np.diag(vec)), Vs)\n",
    "        \n",
    "        # Update data tensor (imputation)\n",
    "        parse_list = [G]\n",
    "        for i in range(dim_N):\n",
    "            parse_list.append(U[i])\n",
    "        X_hat = tucker_combine(parse_list, skip = False)\n",
    "        X_pre = X.copy()\n",
    "        X[pos] = X_hat[pos].copy()\n",
    "\n",
    "        # Update multiplier\n",
    "        for i in range(dim_N):\n",
    "            Y[i] = Y[i] + mu * (ten2mat(G, i) - V[i])\n",
    "\n",
    "#         # Stop criteria\n",
    "#         GVD = []\n",
    "#         for i in range(dim_N):\n",
    "#             GVD.append(np.sum(np.square(ten2mat(G, i) - V[i])))\n",
    "#         tolerance = max(GVD)\n",
    "#         if tolerance < tol:\n",
    "#             break\n",
    "        \n",
    "        # Rank increasing\n",
    "        L = losscal(V, Y, G, U, X, X_pre, mu, lambda_l)\n",
    "        lcr = np.abs(1 - L/L_pre)\n",
    "        L_pre = L\n",
    "        delta_c = delta.copy()\n",
    "        if lcr <= epsilon:\n",
    "            for i in range(dim_N):\n",
    "                delta_c[i] = min(delta[i], R_max[i]-R[i])\n",
    "                if delta_c[i] != 0:\n",
    "                    H = np.random.rand(dim[i], delta_c[i])\n",
    "                    U_hat = np.matmul((np.eye(dim[i]) - np.matmul(U[i], U[i].T)), H)\n",
    "                    U[i] = np.concatenate((U[i], U_hat), axis=1)\n",
    "\n",
    "            R_pre = R.copy()\n",
    "            R = R + delta_c\n",
    "            delta_tuple = ()\n",
    "            for i in range(dim_N):\n",
    "                delta_tuple += ((0, delta_c[i]), )\n",
    "                \n",
    "            for i in range(dim_N):\n",
    "                W_cal = mat2ten(V[i], R_pre, i)\n",
    "                W_cal_c = np.pad(W_cal, delta_tuple, 'constant', constant_values=(0))\n",
    "                V[i] = ten2mat(W_cal_c, i)\n",
    "\n",
    "            for i in range(dim_N):\n",
    "                W_cal = mat2ten(Y[i], R_pre, i)\n",
    "                W_cal_c = np.pad(W_cal, delta_tuple, 'constant', constant_values=(0))\n",
    "                Y[i] = ten2mat(W_cal_c, i)\n",
    "        \n",
    "        # Update parameter mu\n",
    "        mu = max(mu * rho, mu_min)\n",
    "            \n",
    "        if (iteration + 1) % 200 == 0:\n",
    "            print('Iteration: %d, Time cost: %ds'%(iteration + 1, time.time() - start_time))\n",
    "#             print('Tolerance: {:.6}'.format(tolerance))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], X[pos_test])))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], X[pos_test])))\n",
    "            print('Current rank:')\n",
    "            print(R)\n",
    "            print()\n",
    "            start_time = time.time()\n",
    "            \n",
    "    print('Total iteration: %d'%(iteration + 1))\n",
    "#     print('Tolerance: {:.6}'.format(tolerance))\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], X[pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], X[pos_test])))\n",
    "    print('Current rank:')\n",
    "    print(R)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guangzhou data\n",
    "\n",
    "We generate **random missing (RM)** values on Guangzhou traffic speed data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape:\n",
      "(214, 144, 61)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(110)\n",
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('datasets/Guangzhou-data-set/tensor.mat')\n",
    "dense_tensor = tensor['tensor']\n",
    "random_tensor = scipy.io.loadmat('datasets/Guangzhou-data-set/random_tensor.mat')\n",
    "random_tensor = random_tensor['random_tensor']\n",
    "\n",
    "missing_rate = 0.2\n",
    "\n",
    "### Random missing (RM) scenario:\n",
    "binary_tensor = np.round(random_tensor + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "\n",
    "dense_tensor = np.transpose(dense_tensor, [0, 2, 1])\n",
    "sparse_tensor = np.transpose(sparse_tensor, [0, 2, 1])\n",
    "print('Tensor shape:')\n",
    "print(dense_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `imputer` to fill in the missing entries and measure performance metrics on the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200, Time cost: 56s\n",
      "MAPE: 0.0860795\n",
      "RMSE: 3.70923\n",
      "Current rank:\n",
      "[69 35 18]\n",
      "\n",
      "Total iteration: 200\n",
      "Imputation MAPE: 0.0860795\n",
      "Imputation RMSE: 3.70923\n",
      "Current rank:\n",
      "[69 35 18]\n",
      "Running time: 56 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "r = np.array([1, 1, 1])\n",
    "dim = sparse_tensor.shape\n",
    "R_max = np.array(dim)\n",
    "delta = np.array(np.round(dim / np.min(dim)), dtype = 'int')\n",
    "epsilon = 0.05\n",
    "lambda_l = 1\n",
    "rho = 0.95\n",
    "mu0 = 0.0001\n",
    "mu_min = 0.0000001\n",
    "maxiter = 200\n",
    "tensor_hat = imputer(dense_tensor, sparse_tensor, r, R_max, lambda_l, rho, mu0, mu_min, delta, epsilon, maxiter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate **non-random missing (NM)** values on Guangzhou traffic speed data set. Then, we conduct the imputation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('datasets/Guangzhou-data-set/tensor.mat')\n",
    "dense_tensor = tensor['tensor']\n",
    "random_matrix = scipy.io.loadmat('datasets/Guangzhou-data-set/random_matrix.mat')\n",
    "random_matrix = random_matrix['random_matrix']\n",
    "\n",
    "missing_rate = 0.2\n",
    "\n",
    "### Non-random missing (NM) scenario:\n",
    "binary_tensor = np.zeros(dense_tensor.shape)\n",
    "for i1 in range(dense_tensor.shape[0]):\n",
    "    for i2 in range(dense_tensor.shape[1]):\n",
    "        binary_tensor[i1, i2, :] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_tensor = np.multiply(dense_tensor, binary_tensor)\n",
    "\n",
    "dense_tensor = np.transpose(dense_tensor, [0, 2, 1])\n",
    "sparse_tensor = np.transpose(sparse_tensor, [0, 2, 1])\n",
    "\n",
    "del tensor, random_matrix, binary_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "r = np.array([1, 1, 1])\n",
    "dim = sparse_tensor.shape\n",
    "R_max = np.array(dim)\n",
    "delta = np.array(np.round(dim / np.min(dim)), dtype = 'int')\n",
    "epsilon = 0.01\n",
    "lambda_l = 1\n",
    "rho = 0.95\n",
    "mu0 = 0.0001\n",
    "mu_min = 0.0000001\n",
    "maxiter = 200\n",
    "tensor_hat = imputer(dense_tensor, sparse_tensor, r, R_max, lambda_l, rho, mu0, mu_min, delta, epsilon, maxiter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
